{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "在alexnet网络下加上数据增强\n",
    "1.先得到训练集和测试集的图片地址\n",
    "2.将测试集的图片扩展后保存(736)\n",
    "3.将训练集的图片提取特征后保存（2934）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "运行条件：alex_net权值文件、\n",
    "          处理好的图片文件(image_file_name,pkl文件，里面是字典)\n",
    "          image_dict{\n",
    "                          'train':{'image':{....},'label':{}},\n",
    "                        'test':{'image':{....},'label':{}},\n",
    "                    }一般来说存储的时候类型为uint8,提取的时候再转为float32,这样节省空间\n",
    "'''\n",
    "#程序的一些参数设定\n",
    "#设定可以训练与不可训练的网络层\n",
    "#               conv1     conv2      conv3       conv4     conv5    fc6   fc7     fc8\n",
    "variable_trable=[False,    False,     False,    False,    False,  False,  True,  True,]\n",
    "#处理好的图片文件\n",
    "image_file_name='flower.pkl'\n",
    "#权值文件\n",
    "MODEL_ADDR=r'F:\\CLASSIC_MODEL\\ALEXNET\\bvlc_alexnet.npy'\n",
    "#Tensorboard保存的文件名\n",
    "log_file_name='fc7tofc8_DataAugment_4long'\n",
    "#选择是否保存提取出来的特征值\n",
    "save_feature=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#确保下载的权值文件和这个ipython文件再同一个文件夹里面，或者自己指定绝对路径\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "#from class_name import class_names\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#                 conv1               conv2           conv3           conv4             conv5\n",
    "input_shape_list=[[None,227,227,3],[None,27,27,96],[None,13,13,256],[None,13,13,384],[None,13,13,384],\\\n",
    "                 # fc6            fc7           fc8\n",
    "                  [None,9216],[None,4096],[None,4096]\\\n",
    "                 ]\n",
    "for vindex,vi in enumerate(variable_trable):\n",
    "    if vi:\n",
    "        input_shape__=input_shape_list[vindex]\n",
    "        layer_index=vindex\n",
    "        break\n",
    "\n",
    "variable_trable=[None]+variable_trable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(input, kernel, biases, c_o, s_h, s_w, padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    if group == 1:\n",
    "        #不对输入分组卷积\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        #将输入平分成group组，按[N,w,h,channel]->[0,1,2,3]也就是按输入的channel来分成两个矩阵\n",
    "        input_groups = tf.split(input, group, 3)  # tf.split(3, group, input)\n",
    "        #将卷积核平分成group组，按[w,h,in_channel,out_channel]->[0,1,2,3]也就是按输入的channel来分成两个矩阵\n",
    "        kernel_groups = tf.split(kernel, group, 3)  # tf.split(3, group, kernel)\n",
    "        #分组卷积\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "        #连接卷积的结果\n",
    "        conv = tf.concat(output_groups, 3)  # tf.concat(3, output_groups)\n",
    "    return conv + biases\n",
    "\n",
    "def lrn(x):\n",
    "    #return x\n",
    "    #lrn层，现在比较少用，一般用bn层代替\n",
    "    return tf.nn.local_response_normalization(x,\n",
    "                                              depth_radius=2,\n",
    "                                              alpha=2e-05,\n",
    "                                              beta=0.75,\n",
    "                                              bias=1.0)\n",
    "def maxpool(x):\n",
    "    #因为alex net 用到的maxpool都是一样的参数，所以直接写以函数代替，不用填参数\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def load_model_weight_and_biases():\n",
    "    '''\n",
    "    读取模型中的变量值，返回训练好的权重\n",
    "    model_addr：模型的路径\n",
    "    '''\n",
    "    weights_dict = np.load(MODEL_ADDR, encoding='bytes').item()\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alexnet(x,net_data,keep_prob,bn_training=True,train=True,extrater_layer=8):#提取特征的时候train=False\n",
    "    '''\n",
    "    train=False:特征提取模式，会跑整个model\n",
    "    train=True:训练模式，只跑需要训练的层\n",
    "    '''\n",
    "    input_flag=train\n",
    "    op_list=[]\n",
    "    #layer_1 conv1-relu-lrn-maxpool\n",
    "    if variable_trable[1] or (not train):# 0 or 1\n",
    "        with tf.name_scope('layer_1'):\n",
    "            CONV1_W,CONV1_b=tf.Variable(net_data['conv1'][0],name='conv1_w',trainable=variable_trable[1]),\\\n",
    "            tf.Variable(net_data['conv1'][1],name='conv1_b',trainable=variable_trable[1])\n",
    "            conv1_=conv(X, CONV1_W, CONV1_b, c_o=96, s_h=4, s_w=4, padding=\"VALID\", group=1)\n",
    "            relu1_=tf.nn.relu(conv1_)\n",
    "            norm1=lrn(relu1_)#55*55*96\n",
    "            maxpool1_=maxpool(norm1)#27*27*96\n",
    "            if not train:op_list.append(maxpool1_)\n",
    "    #layer_2 conv2-relu-lrn-maxpool\n",
    "    if variable_trable[2] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool1_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_2'):\n",
    "            CONV2_W,CONV2_b=tf.Variable(net_data['conv2'][0],name='conv2_w',trainable=variable_trable[2]), \\\n",
    "            tf.Variable(net_data['conv2'][1],name='conv2_b',trainable=variable_trable[2])\n",
    "            conv2_=conv(maxpool1_, CONV2_W, CONV2_b, c_o=256, s_h=1, s_w=1, padding=\"SAME\", group=2)#27*27*256\n",
    "            relu2_=tf.nn.relu(conv2_)\n",
    "            norm2=lrn(relu2_)\n",
    "            maxpool2_=maxpool(norm2)\n",
    "            if not train:op_list.append(maxpool2_)#13*13*256\n",
    "        \n",
    "    #layer_3 conv3-relu\n",
    "    if variable_trable[3] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool2_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_3'):\n",
    "            CONV3_W,CONV3_b=tf.Variable(net_data['conv3'][0],name='conv3_w',trainable=variable_trable[3]),\\\n",
    "            tf.Variable(net_data['conv3'][1],name='conv3_b',trainable=variable_trable[3])\n",
    "            conv3_=conv(maxpool2_, CONV3_W, CONV3_b, c_o=384, s_h=1, s_w=1, padding=\"SAME\", group=1)#13*13*384\n",
    "            relu3_=tf.nn.relu(conv3_)\n",
    "            if not train:op_list.append(relu3_)#13*13*384\n",
    "    #layer_4 conv4-relu\n",
    "    if variable_trable[4] or (not train):\n",
    "        if input_flag:\n",
    "            relu3_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_4'):\n",
    "            CONV4_W,CONV4_b=tf.Variable(net_data['conv4'][0],name='conv4_w',trainable=variable_trable[4]), \\\n",
    "            tf.Variable(net_data['conv4'][1],name='conv4_b',trainable=variable_trable[4])\n",
    "            conv4_=conv(relu3_, CONV4_W, CONV4_b, c_o=384, s_h=1, s_w=1, padding=\"SAME\", group=2)#13*13*384\n",
    "            relu4_=tf.nn.relu(conv4_)\n",
    "            if not train:op_list.append(relu4_)#13*13*384\n",
    "    \n",
    "    #layer_5 conv5-relu-maxpool\n",
    "    if variable_trable[5] or (not train):\n",
    "        if input_flag:\n",
    "            relu4_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_5'):\n",
    "            CONV5_W,CONV5_b=tf.Variable(net_data['conv5'][0],name='conv5_w',trainable=variable_trable[5]), \\\n",
    "            tf.Variable(net_data['conv5'][1],name='conv5_b',trainable=variable_trable[5])\n",
    "            conv5_=conv(relu4_, CONV5_W, CONV5_b, c_o=256, s_h=1, s_w=1, padding=\"SAME\", group=2)\n",
    "            relu5_=tf.nn.relu(conv5_)#13*13*256\n",
    "            maxpool5_=maxpool(relu5_)\n",
    "            if not train:op_list.append(maxpool5_)#6*6*256\n",
    "    if variable_trable[6] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool5_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_6'):\n",
    "            floatten_input=tf.reshape(maxpool5_,[-1,9216])#N*9216\n",
    "            floatten_input=tf.nn.dropout(x=floatten_input,keep_prob=keep_prob)\n",
    "            fc6_w,fc6_b=tf.Variable(net_data['fc6'][0],name='fc6_w',trainable=variable_trable[6]), \\\n",
    "            tf.Variable(net_data['fc6'][1],name='fc7_b',trainable=variable_trable[6])\n",
    "            fc6_=tf.matmul(floatten_input,fc6_w)+fc6_b\n",
    "            relu6_=tf.nn.relu(fc6_)#N*4096\n",
    "            if not train:op_list.append(relu6_)\n",
    "    if variable_trable[7] or (not train):\n",
    "        if input_flag:\n",
    "            relu6_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_7'):\n",
    "            relu6_=tf.nn.dropout(x=relu6_,keep_prob=keep_prob)\n",
    "            fc7_w,fc7_b=tf.Variable(net_data['fc7'][0],name='fc7_w',trainable=variable_trable[7]),\\\n",
    "            tf.Variable(net_data['fc7'][1],name='fc7_b',trainable=variable_trable[7])\n",
    "            fc7_=tf.matmul(relu6_,fc7_w)+fc7_b\n",
    "            tf.layers.batch_normalization(fc7_,training=bn_training)\n",
    "            relu7_=tf.nn.relu(fc7_)#N*4096\n",
    "            #加入tensor的直方图监视\n",
    "            tf.summary.histogram('fc7_W',fc7_w)\n",
    "            tf.summary.histogram('fc7_b',fc7_b)\n",
    "            tf.summary.histogram('fc7_Wx_plus_b',fc7_)\n",
    "            tf.summary.histogram('fc7_activate',relu7_)\n",
    "            if not train:op_list.append(relu7_)\n",
    "            \n",
    "    if variable_trable[8] or (not train):\n",
    "        if input_flag:\n",
    "            relu7_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_8'):\n",
    "    #         fc8_w,fc8_b=tf.Variable(net_data['fc8'][0],name='fc8_w',trainable=variable_trable[8]), \\\n",
    "    #         tf.Variable(net_data['fc8'][1],name='fc8_b',trainable=variable_trable[8])\n",
    "            relu7_=tf.nn.dropout(x=relu7_,keep_prob=keep_prob)\n",
    "            #最后一层fc层必须要重新训练\n",
    "            fc8_w=tf.Variable(tf.truncated_normal(shape=[4096,5],stddev=0.01),dtype=tf.float32,name='fc8_w',\\\n",
    "                              trainable=variable_trable[8])\n",
    "            fc8_b=tf.Variable(tf.zeros(shape=[5]),dtype=tf.float32,name='fc8_b',trainable=variable_trable[8])\n",
    "            fc8_=tf.matmul(relu7_,fc8_w)+fc8_b#N*1000\n",
    "            #添加权值的tensorboard监视\n",
    "            tf.summary.histogram('fc8_W',fc8_w)\n",
    "            tf.summary.histogram('fc8_b',fc8_b)\n",
    "            \n",
    "            if not train:op_list.append(fc8_)\n",
    "    #     return fc8_\n",
    "    return fc8_ if train else op_list[extrater_layer-1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_pen=tf.reduce_sum([tf.reduce_sum(tf.square(i)) for i in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netdata=load_model_weight_and_biases()\n",
    "with tf.name_scope('input'):\n",
    "    X=tf.placeholder(dtype=tf.float32,shape=[None,227,227,3])\n",
    "    Y=tf.placeholder(dtype=tf.float32,shape=[None,5])\n",
    "    FX=tf.placeholder(dtype=tf.float32,shape=input_shape__)\n",
    "    KEEP_PROB=tf.placeholder(dtype=tf.float32)\n",
    "    LEARNRATE=tf.placeholder(dtype=tf.float32)\n",
    "    BN_TRANING=tf.placeholder(tf.bool)#BN占位符，训练标志位\n",
    "with tf.name_scope('predict'):\n",
    "    y_pre=alexnet(FX,netdata,KEEP_PROB)\n",
    "    #prob=tf.nn.softmax(y_pre)\n",
    "    #修改测试集，对于一张测试集扩展成10张，计算平均得分,测试集取32张图片\n",
    "    #转换成batch size的平均分\n",
    "    mean_score=tf.reduce_mean(tf.reshape(y_pre,[-1,10,5]),axis=1)\n",
    "    mean_lable=tf.reduce_mean(tf.reshape(Y,[-1,10,5]),axis=1)\n",
    "    acc_c_t=tf.equal(tf.arg_max(mean_score,1),tf.arg_max(mean_lable,1))\n",
    "    accuracy_t=tf.reduce_mean(tf.cast(x=acc_c_t,dtype=tf.float32))#测试集准确率，loss\n",
    "    loss_test=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=mean_score,labels=mean_lable))\n",
    "#在不需要抽取特征的时候注释extrator网络，因为 extrator和predict是两组不一样的网络，这样能节省显存\n",
    "# with tf.name_scope('extrator'):\n",
    "#     feature=alexnet(X,netdata,KEEP_PROB,train=False,extrater_layer=layer_index)\n",
    "#loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pre,labels=Y))\n",
    "    loss+=7e-3*reg_pen\n",
    "with tf.name_scope('trainer'):\n",
    "    trainer=tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc_c=tf.equal(tf.arg_max(y_pre,1),tf.arg_max(Y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(x=acc_c,dtype=tf.float32))\n",
    "sess=tf.InteractiveSession()\n",
    "log_root=r'D:\\Data warehouse\\temp_dump\\mylog'\n",
    "writer=tf.summary.FileWriter(log_root)\n",
    "init=tf.global_variables_initializer()\n",
    "log_file_name='train_'+log_file_name\n",
    "writer=tf.summary.FileWriter(os.path.join(log_root,log_file_name))\n",
    "log_file_name='test_'+log_file_name[6:]\n",
    "writer_=tf.summary.FileWriter(os.path.join(log_root,log_file_name))\n",
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "merge=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#下面这部分为制作训练集，测试集，如果已存在不用运行这部分程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**************************************************************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "下面将训练集图片转换成特征：\n",
    "1.得到一张训练图片\n",
    "2.扩展200倍\n",
    "3.送入alexnet网络提取特征\n",
    "4.保存\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FlowerData as fd\n",
    "import numpy as np\n",
    "import os\n",
    "import data_augmentation as da\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_file_root=r'D:\\Data warehouse\\5 flower\\flower_photos'\n",
    "#先用图片地址来划分训练集，测试集\n",
    "img_addr_dict=fd.GetFlowerPicAddr(img_file_root)\n",
    "fp=open(r'D:\\Data warehouse\\temp_dump\\flower_dict.pkl','wb')\n",
    "#把这个划分好的地址存起来，后面多次会用到\n",
    "#（因为划分的mask是随机的，所以为了避免测试与训练集合重复，必须用同一个地址字典）\n",
    "pickle.dump(file=fp,obj=img_addr_dict)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp=open(r'D:\\Data warehouse\\temp_dump\\flower_dict.pkl','rb')\n",
    "img_addr_dict=pickle.load(fp)\n",
    "fp.close()\n",
    "feature_mat,label_mat=[],[]\n",
    "picture_counter=0\n",
    "counter=0\n",
    "batch_num=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in fd.YieldTrainImage(img_addr_dict):#yield一次一次的加载，避免全部加载给内存太大压力\n",
    "    x_,y_=map(np.asarray,i)\n",
    "    TR_FEA=sess.run(feature,feed_dict={X:x_,Y:y_,KEEP_PROB:1.})#转换成特征\n",
    "    TR_FEA=TR_FEA.reshape((200,-1))\n",
    "    feature_mat.append(TR_FEA)\n",
    "    label_mat.append(y_)\n",
    "    counter+=1\n",
    "    picture_counter+=1\n",
    "    if counter%100==0:\n",
    "        print('batch counter:{},picturenum:{}/2934,batch num:{}'.format(counter,picture_counter,batch_num))\n",
    "    #按batch保存,每个batch保存 200*400=80000条特征，最后一个batch大概20000条\n",
    "    if counter>80*5:\n",
    "        counter=0\n",
    "        batch_num+=1\n",
    "        fea_filename='feature_batch_'+str(batch_num)+'.pkl'\n",
    "        froot=r'D:\\Data warehouse\\temp_dump\\feature_batch'\n",
    "        feature_mat,label_mat=map(np.concatenate,[feature_mat,label_mat])\n",
    "        fp=open(os.path.join(froot,fea_filename),'wb')\n",
    "        idict={'feature':feature_mat.astype(np.float32),'lable':label_mat.astype('uint8')}\n",
    "        pickle.dump(file=fp,obj=idict)\n",
    "        fp.close()\n",
    "        del idict\n",
    "        feature_mat,label_mat=[],[]\n",
    "        print('---batch num:{} save,picturenum:{}/2934'.format(batch_num,picture_counter))\n",
    "#最后一个batch\n",
    "if not counter == 0:\n",
    "    batch_num+=1\n",
    "    fea_filename='feature_batch_'+str(batch_num)+'.pkl'\n",
    "    froot=r'D:\\Data warehouse\\temp_dump\\feature_batch'\n",
    "    feature_mat,label_mat=map(np.concatenate,[feature_mat,label_mat])\n",
    "    fp=open(os.path.join(froot,fea_filename),'wb')\n",
    "    idict={'feature':feature_mat.astype(np.float16),'lable':label_mat.astype('uint8')}\n",
    "    pickle.dump(file=fp,obj=idict)\n",
    "    fp.close()\n",
    "    del idict\n",
    "    feature_mat,label_mat=[],[]\n",
    "    print('---batch num:{} save,picturenum:{}/2934'.format(batch_num,picture_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "下面制作测试集\n",
    "'''\n",
    "#读取地址\n",
    "fp=open(r'D:\\Data warehouse\\temp_dump\\flower_dict.pkl','rb')\n",
    "img_dict=pickle.load(fp)\n",
    "fp.close()\n",
    "#保存扩展后的测试图片\n",
    "fd.SaveTestImage(img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#抽取测试集特征\n",
    "fp=open(r'D:\\Data warehouse\\temp_dump\\flower_test_img.pkl','rb')\n",
    "test_imgdict=pickle.load(fp)\n",
    "fp.close()\n",
    "test_feature={'feature':None,'lables':None}\n",
    "test_feature_mat,test_feature_lable=[],[]\n",
    "print('converting test image to feature!')\n",
    "#\n",
    "test_feature_mat=[]\n",
    "test_feature_lable=[]\n",
    "for fname in test_imgdict:#按种类，每10张（由一张原图扩展得来）送入网络抽取特征\n",
    "    x_all=test_imgdict[fname]['image'].astype(np.float32)\n",
    "    y_all=test_imgdict[fname]['lables'].astype(np.float32)\n",
    "    pic_num=x_all.shape[0]\n",
    "    print(fname,pic_num)\n",
    "    #一张图扩展成10张，每10张输入网络提取\n",
    "    loop_num=pic_num//10\n",
    "    s_index=0\n",
    "    for l_ in range(loop_num):\n",
    "        TR_FEA=sess.run(feature,feed_dict={X:x_all[s_index:s_index+10],Y:y_all[s_index:s_index+10],KEEP_PROB:1.})\n",
    "        TR_FEA=TR_FEA.reshape((1,10,-1))#第一维度仍然保持为 736，方便取样\n",
    "        test_feature_mat.append(TR_FEA)\n",
    "        s_index+=10\n",
    "        test_feature_lable.append(test_imgdict[fname]['lables'][:10].reshape((1,10,5)))\n",
    "test_feature_mat,test_feature_lable=map(np.concatenate,[test_feature_mat,test_feature_lable])\n",
    "test_feature_dict={'feature':test_feature_mat,'lable':test_feature_lable}\n",
    "fp=open(r'D:\\Data warehouse\\temp_dump\\feature_batch\\test_feature.pkl','wb')\n",
    "pickle.dump(file=fp,obj=test_feature_dict)\n",
    "fp.close()\n",
    "print('save test feature!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/*************************************************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp=open(r'D:\\Data warehouse\\temp_dump\\feature_batch\\test_feature.pkl','rb')\n",
    "test_fea=pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 10, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fea['lable'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bn操作符\n",
    "bn_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#……………………………………训练网络……………………………………………………\n",
    "#%debug\n",
    "kp_te,kp_tr=1.,0.5\n",
    "lr=1e-3\n",
    "run_counter=0\n",
    "for i in range(8*100):\n",
    "    print('load new batch!!')\n",
    "    batch_id=np.random.choice(list(range(1,9)),3,replace=False)\n",
    "    train_fea={'feature':[],'lable':[]}\n",
    "    #加载3个batch文件到内存,每个batch有400*200=80000条特征\n",
    "    for t_id in batch_id:\n",
    "        fea_filename='feature_batch_'+str(t_id)+'.pkl'\n",
    "        froot=r'D:\\Data warehouse\\temp_dump\\feature_batch'\n",
    "        fp=open(os.path.join(froot,fea_filename),'rb')\n",
    "        t_fea=pickle.load(fp)\n",
    "        fp.close()\n",
    "        train_fea['feature'].append(t_fea['feature'])\n",
    "        train_fea['lable'].append(t_fea['lable'])\n",
    "    train_fea['feature']=np.concatenate(train_fea['feature'])\n",
    "    train_fea['lable']=np.concatenate(train_fea['lable'])\n",
    "    #k*10*10*N\n",
    "    for k in range(62*5):\n",
    "        mask=np.random.choice(train_fea['lable'].shape[0],512,replace=False)\n",
    "        x_,y_=train_fea['feature'][mask].astype(np.float32),train_fea['lable'][mask].astype(np.float32)\n",
    "        \n",
    "        for j in range(10):\n",
    "            run_counter+=1\n",
    "            sess.run([trainer,bn_ops],feed_dict={FX:x_,Y:y_,KEEP_PROB:kp_tr,LEARNRATE:lr,BN_TRANING:True})\n",
    "        if run_counter%100==0:\n",
    "            loss_,acc_,m_=sess.run([loss,accuracy,merge],feed_dict={FX:x_,Y:y_,KEEP_PROB:kp_tr,LEARNRATE:lr})\n",
    "            writer.add_summary(m_,run_counter)\n",
    "            print('epoch:{},loss:{},train accuracy:{}'.format(run_counter,loss_,acc_))\n",
    "        if run_counter%(200*3)==0:\n",
    "            mask=np.random.choice(test_fea['feature'].shape[0],128,replace=False)\n",
    "            #32*10*227*227*3,32*5\n",
    "            x_,y_=(test_fea['feature'][mask]).reshape((1280,-1)),test_fea['lable'][mask].reshape((1280,-1))\n",
    "            acc_,loss_,m_,_=sess.run([accuracy_t,loss_test,merge,bn_ops],\\\n",
    "                                     feed_dict={FX:x_,Y:y_,KEEP_PROB:1.,LEARNRATE:lr,BN_TRANING:False})\n",
    "            writer_.add_summary(m_,run_counter)\n",
    "            print('--epoch:{}/3760000,test loss:{},test accuracy:{}'.format(run_counter,loss_,acc_))\n",
    "    if i%8==0:\n",
    "        lr=max(1e-5,0.99*lr)\n",
    "        print('--epoch:{},New learning rate:{}'.format(run_counter,lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea['feature'][mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()\n",
    "writer_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp=open(r'D:\\Data warehouse\\temp_dump\\feature_batch\\test_feature.pkl','rb')\n",
    "test_fea=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fea['feature'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in test_fea:\n",
    "    c+=test_fea['feature'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mask=np.random.choice(test_fea['feature'].shape[0],128,replace=False)\n",
    "x_,y_=test_fea['feature'][100:200],test_fea['lable'][100:200]\n",
    "acc_=sess.run(mean_lable,feed_dict={FX:x_,Y:y_,KEEP_PROB:1.,LEARNRATE:1e-4})\n",
    "print(acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
