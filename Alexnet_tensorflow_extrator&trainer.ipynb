{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "运行条件：alex_net权值文件、\n",
    "          处理好的图片文件(image_file_name,pkl文件，里面是字典)\n",
    "          image_dict{\n",
    "                          'train':{'image':{....},'label':{}},\n",
    "                        'test':{'image':{....},'label':{}},\n",
    "                    }一般来说存储的时候类型为uint8,提取的时候再转为float32,这样节省空间\n",
    "'''\n",
    "#程序的一些参数设定\n",
    "#设定可以训练与不可训练的网络层\n",
    "#               conv1     conv2      conv3       conv4     conv5    fc6   fc7     fc8\n",
    "variable_trable=[False,    False,     False,    False,    False,  True,  True,  True,]\n",
    "#处理好的图片文件\n",
    "image_file_name='flower.pkl'\n",
    "#权值文件\n",
    "MODEL_ADDR='bvlc_alexnet.npy'\n",
    "#Tensorboard保存的文件名\n",
    "log_file_name='conv3tofc8_modify'\n",
    "#选择是否保存提取出来的特征值\n",
    "save_feature=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#确保下载的权值文件和这个ipython文件再同一个文件夹里面，或者自己指定绝对路径\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from class_name import class_names\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#                 conv1               conv2           conv3           conv4             conv5\n",
    "input_shape_list=[[None,227,227,3],[None,27,27,96],[None,13,13,256],[None,13,13,384],[None,13,13,384],\\\n",
    "                 # fc6            fc7           fc8\n",
    "                  [None,6,6,256],[None,4096],[None,4096]\\\n",
    "                 ]\n",
    "for vindex,vi in enumerate(variable_trable):\n",
    "    if vi:\n",
    "        input_shape__=input_shape_list[vindex]\n",
    "        layer_index=vindex\n",
    "        break\n",
    "\n",
    "variable_trable=[None]+variable_trable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(input, kernel, biases, c_o, s_h, s_w, padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    if group == 1:\n",
    "        #不对输入分组卷积\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        #将输入平分成group组，按[N,w,h,channel]->[0,1,2,3]也就是按输入的channel来分成两个矩阵\n",
    "        input_groups = tf.split(input, group, 3)  # tf.split(3, group, input)\n",
    "        #将卷积核平分成group组，按[w,h,in_channel,out_channel]->[0,1,2,3]也就是按输入的channel来分成两个矩阵\n",
    "        kernel_groups = tf.split(kernel, group, 3)  # tf.split(3, group, kernel)\n",
    "        #分组卷积\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "        #连接卷积的结果\n",
    "        conv = tf.concat(output_groups, 3)  # tf.concat(3, output_groups)\n",
    "    return conv + biases\n",
    "\n",
    "def lrn(x):\n",
    "    #return x\n",
    "    #lrn层，现在比较少用，一般用bn层代替\n",
    "    return tf.nn.local_response_normalization(x,\n",
    "                                              depth_radius=2,\n",
    "                                              alpha=2e-05,\n",
    "                                              beta=0.75,\n",
    "                                              bias=1.0)\n",
    "def maxpool(x):\n",
    "    #因为alex net 用到的maxpool都是一样的参数，所以直接写以函数代替，不用填参数\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def load_model_weight_and_biases():\n",
    "    '''\n",
    "    读取模型中的变量值，返回训练好的权重\n",
    "    model_addr：模型的路径\n",
    "    '''\n",
    "    weights_dict = np.load(MODEL_ADDR, encoding='bytes').item()\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alexnet(x,net_data,keep_prob,train=True,extrater_layer=8):#提取特征的时候train=False\n",
    "    '''\n",
    "    train=False:特征提取模式，会跑整个model\n",
    "    train=True:训练模式，只跑需要训练的层\n",
    "    '''\n",
    "    input_flag=train\n",
    "    op_list=[]\n",
    "    #layer_1 conv1-relu-lrn-maxpool\n",
    "    if variable_trable[1] or (not train):# 0 or 1\n",
    "        with tf.name_scope('layer_1'):\n",
    "            CONV1_W,CONV1_b=tf.Variable(net_data['conv1'][0],name='conv1_w',trainable=variable_trable[1]),\\\n",
    "            tf.Variable(net_data['conv1'][1],name='conv1_b',trainable=variable_trable[1])\n",
    "            conv1_=conv(X, CONV1_W, CONV1_b, c_o=96, s_h=4, s_w=4, padding=\"VALID\", group=1)\n",
    "            relu1_=tf.nn.relu(conv1_)\n",
    "            norm1=lrn(relu1_)#55*55*96\n",
    "            maxpool1_=maxpool(norm1)#27*27*96\n",
    "            if not train:op_list.append(maxpool1_)\n",
    "    #layer_2 conv2-relu-lrn-maxpool\n",
    "    if variable_trable[2] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool1_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_2'):\n",
    "            CONV2_W,CONV2_b=tf.Variable(net_data['conv2'][0],name='conv2_w',trainable=variable_trable[2]), \\\n",
    "            tf.Variable(net_data['conv2'][1],name='conv2_b',trainable=variable_trable[2])\n",
    "            conv2_=conv(maxpool1_, CONV2_W, CONV2_b, c_o=256, s_h=1, s_w=1, padding=\"SAME\", group=2)#27*27*256\n",
    "            relu2_=tf.nn.relu(conv2_)\n",
    "            norm2=lrn(relu2_)\n",
    "            maxpool2_=maxpool(norm2)\n",
    "            if not train:op_list.append(maxpool2_)#13*13*256\n",
    "        \n",
    "    #layer_3 conv3-relu\n",
    "    if variable_trable[3] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool2_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_3'):\n",
    "            CONV3_W,CONV3_b=tf.Variable(net_data['conv3'][0],name='conv3_w',trainable=variable_trable[3]),\\\n",
    "            tf.Variable(net_data['conv3'][1],name='conv3_b',trainable=variable_trable[3])\n",
    "            conv3_=conv(maxpool2_, CONV3_W, CONV3_b, c_o=384, s_h=1, s_w=1, padding=\"SAME\", group=1)#13*13*384\n",
    "            relu3_=tf.nn.relu(conv3_)\n",
    "            if not train:op_list.append(relu3_)#13*13*384\n",
    "    #layer_4 conv4-relu\n",
    "    if variable_trable[4] or (not train):\n",
    "        if input_flag:\n",
    "            relu3_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_4'):\n",
    "            CONV4_W,CONV4_b=tf.Variable(net_data['conv4'][0],name='conv4_w',trainable=variable_trable[4]), \\\n",
    "            tf.Variable(net_data['conv4'][1],name='conv4_b',trainable=variable_trable[4])\n",
    "            conv4_=conv(relu3_, CONV4_W, CONV4_b, c_o=384, s_h=1, s_w=1, padding=\"SAME\", group=2)#13*13*384\n",
    "            relu4_=tf.nn.relu(conv4_)\n",
    "            if not train:op_list.append(relu4_)#13*13*384\n",
    "    \n",
    "    #layer_5 conv5-relu-maxpool\n",
    "    if variable_trable[5] or (not train):\n",
    "        if input_flag:\n",
    "            relu4_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_5'):\n",
    "            CONV5_W,CONV5_b=tf.Variable(net_data['conv5'][0],name='conv5_w',trainable=variable_trable[5]), \\\n",
    "            tf.Variable(net_data['conv5'][1],name='conv5_b',trainable=variable_trable[5])\n",
    "            conv5_=conv(relu4_, CONV5_W, CONV5_b, c_o=256, s_h=1, s_w=1, padding=\"SAME\", group=2)\n",
    "            relu5_=tf.nn.relu(conv5_)#13*13*256\n",
    "            maxpool5_=maxpool(relu5_)\n",
    "            if not train:op_list.append(maxpool5_)#6*6*256\n",
    "    if variable_trable[6] or (not train):\n",
    "        if input_flag:\n",
    "            maxpool5_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_6'):\n",
    "            floatten_input=tf.reshape(maxpool5_,[-1,9216])#N*9216\n",
    "            floatten_input=tf.nn.dropout(x=floatten_input,keep_prob=keep_prob)\n",
    "            fc6_w,fc6_b=tf.Variable(net_data['fc6'][0],name='fc6_w',trainable=variable_trable[6]), \\\n",
    "            tf.Variable(net_data['fc6'][1],name='fc7_b',trainable=variable_trable[6])\n",
    "            fc6_=tf.matmul(floatten_input,fc6_w)+fc6_b\n",
    "            relu6_=tf.nn.relu(fc6_)#N*4096\n",
    "            if not train:op_list.append(relu6_)\n",
    "    if variable_trable[7] or (not train):\n",
    "        if input_flag:\n",
    "            relu6_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_7'):\n",
    "            relu6_=tf.nn.dropout(x=relu6_,keep_prob=keep_prob)\n",
    "            fc7_w,fc7_b=tf.Variable(net_data['fc7'][0],name='fc7_w',trainable=variable_trable[7]),\\\n",
    "            tf.Variable(net_data['fc7'][1],name='fc7_b',trainable=variable_trable[7])\n",
    "            fc7_=tf.matmul(relu6_,fc7_w)+fc7_b\n",
    "            relu7_=tf.nn.relu(fc7_)#N*4096\n",
    "            if not train:op_list.append(relu7_)\n",
    "    if variable_trable[8] or (not train):\n",
    "        if input_flag:\n",
    "            relu7_=x\n",
    "            input_flag=False\n",
    "        with tf.name_scope('layer_8'):\n",
    "    #         fc8_w,fc8_b=tf.Variable(net_data['fc8'][0],name='fc8_w',trainable=variable_trable[8]), \\\n",
    "    #         tf.Variable(net_data['fc8'][1],name='fc8_b',trainable=variable_trable[8])\n",
    "            relu7_=tf.nn.dropout(x=relu7_,keep_prob=keep_prob)\n",
    "            #最后一层fc层必须要重新训练\n",
    "            fc8_w=tf.Variable(tf.truncated_normal(shape=[4096,5],stddev=0.01),dtype=tf.float32,name='fc8_w',\\\n",
    "                              trainable=variable_trable[8])\n",
    "            fc8_b=tf.Variable(tf.zeros(shape=[5]),dtype=tf.float32,name='fc8_b',trainable=variable_trable[8])\n",
    "            fc8_=tf.matmul(relu7_,fc8_w)+fc8_b#N*1000\n",
    "            if not train:op_list.append(fc8_)\n",
    "    #     return fc8_\n",
    "    return fc8_ if train else op_list[extrater_layer-1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_pen=tf.reduce_sum([tf.reduce_sum(tf.square(i)) for i in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netdata=load_model_weight_and_biases()\n",
    "with tf.name_scope('input'):\n",
    "    X=tf.placeholder(dtype=tf.float32,shape=[None,227,227,3])\n",
    "    Y=tf.placeholder(dtype=tf.float32,shape=[None,5])\n",
    "    FX=tf.placeholder(dtype=tf.float32,shape=input_shape__)\n",
    "    KEEP_PROB=tf.placeholder(dtype=tf.float32)\n",
    "    LEARNRATE=tf.placeholder(dtype=tf.float32)\n",
    "with tf.name_scope('predict'):\n",
    "    y_pre=alexnet(FX,netdata,KEEP_PROB)\n",
    "    prob=tf.nn.softmax(y_pre)\n",
    "with tf.name_scope('extrator'):\n",
    "    feature=alexnet(X,netdata,KEEP_PROB,train=False,extrater_layer=layer_index)\n",
    "#loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pre,labels=Y))\n",
    "    loss+=1e-4*reg_pen\n",
    "with tf.name_scope('trainer'):\n",
    "    trainer=tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc_c=tf.equal(tf.arg_max(y_pre,1),tf.arg_max(Y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(x=acc_c,dtype=tf.float32))\n",
    "sess=tf.InteractiveSession()\n",
    "writer=tf.summary.FileWriter('./mylog',sess.graph)\n",
    "init=tf.global_variables_initializer()\n",
    "log_file_name='train_'+log_file_name\n",
    "writer=tf.summary.FileWriter(os.path.join('mylog',log_file_name))\n",
    "log_file_name='test_'+log_file_name[6:]\n",
    "writer_=tf.summary.FileWriter(os.path.join('mylog',log_file_name))\n",
    "tf.summary.scalar('loss',loss)\n",
    "tf.summary.scalar('accuracy',accuracy)\n",
    "merge=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fp=open(image_file_name,'rb')\n",
    "flower_dict=pickle.load(fp)\n",
    "\n",
    "TR_IMG,TE_IMG=flower_dict['train']['image'].astype(np.float32),flower_dict['test']['image'].astype(np.float32)\n",
    "TR_LAB,TE_LAB=flower_dict['train']['label'].astype(np.float32),flower_dict['test']['label'].astype(np.float32)\n",
    "\n",
    "del flower_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pic_num=TR_IMG.shape[0]\n",
    "batch_size=50\n",
    "mod_num=train_pic_num%batch_size\n",
    "loop_num=train_pic_num//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start conver train image to feature\n",
      "conver image to feature:0/58\n",
      "conver image to feature:10/58\n",
      "conver image to feature:20/58\n",
      "conver image to feature:30/58\n",
      "conver image to feature:40/58\n",
      "conver image to feature:50/58\n"
     ]
    }
   ],
   "source": [
    "TR_FEA=sess.run(feature,feed_dict={X:TR_IMG[:mod_num],Y:TR_LAB[:mod_num],KEEP_PROB:1.})\n",
    "strat_index=mod_num\n",
    "print('start conver train image to feature')\n",
    "for i in range(loop_num):\n",
    "    if i%10==0:\n",
    "        print ('conver image to feature:{}/{}'.format(i,loop_num))\n",
    "    end_index=strat_index+batch_size\n",
    "    x_,y_=TR_IMG[strat_index:end_index],TR_LAB[strat_index:end_index]\n",
    "    strat_index=end_index\n",
    "    f_=sess.run(feature,feed_dict={X:x_,Y:y_,KEEP_PROB:1.})\n",
    "    TR_FEA=np.vstack((TR_FEA,f_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TR_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start conver test image to feature\n",
      "conver image to feature:0/14\n",
      "conver image to feature:10/14\n"
     ]
    }
   ],
   "source": [
    "train_pic_num=TE_IMG.shape[0]\n",
    "batch_size=50\n",
    "mod_num=train_pic_num%batch_size\n",
    "loop_num=train_pic_num//batch_size\n",
    "TE_FEA=sess.run(feature,feed_dict={X:TE_IMG[:mod_num],Y:TE_LAB[:mod_num],KEEP_PROB:1.})\n",
    "strat_index=mod_num\n",
    "print('start conver test image to feature')\n",
    "for i in range(loop_num):\n",
    "    if i%10==0:\n",
    "        print ('conver image to feature:{}/{}'.format(i,loop_num))\n",
    "    end_index=strat_index+batch_size\n",
    "    x_,y_=TE_IMG[strat_index:end_index],TE_LAB[strat_index:end_index]\n",
    "    strat_index=end_index\n",
    "    f_=sess.run(feature,feed_dict={X:x_,Y:y_,KEEP_PROB:1.})\n",
    "    TE_FEA=np.vstack((TE_FEA,f_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del TE_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving feature file ......\n",
      "save feature file done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#……………………………………存储特征值……………………………………………………\n",
    "if save_feature:\n",
    "    print('saving feature file ......')\n",
    "    feature_root_file=r'feature_file'\n",
    "    feature_file=os.path.join(feature_root_file,('image_feature_layer'+str(layer_index)+'output.pkl'))\n",
    "    fp=open(feature_file,'wb')\n",
    "    feature_dict={'train':{},'test':{}}\n",
    "    feature_dict['train']['feature']=TR_FEA.astype(np.float32)\n",
    "    feature_dict['test']['feature']=TE_FEA.astype(np.float32)\n",
    "    feature_dict['train']['label']=TR_LAB.astype(np.int8)\n",
    "    feature_dict['test']['label']=TE_LAB.astype(np.int8)\n",
    "    pickle.dump(file=fp,obj=feature_dict)\n",
    "    fp.close()\n",
    "    del feature_dict\n",
    "    print ('save feature file done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#……………………………………训练网络……………………………………………………\n",
    "kp_te,kp_tr=1.,0.5\n",
    "#sess.run(init)\n",
    "lr=1e-3\n",
    "for i in range(1000):\n",
    "    mask=np.random.choice(TR_FEA.shape[0],128,replace=False)\n",
    "    x_,y_=TR_FEA[mask],TR_LAB[mask]\n",
    "    \n",
    "    loss_,acc_,m_=sess.run([loss,accuracy,merge],feed_dict={FX:x_,Y:y_,KEEP_PROB:kp_tr,LEARNRATE:lr})\n",
    "    writer.add_summary(m_,i)\n",
    "    print('epoch:{},loss:{},train accuracy:{}'.format(i,loss_,acc_))\n",
    "    for j in range(10):\n",
    "        sess.run(trainer,feed_dict={FX:x_,Y:y_,KEEP_PROB:kp_tr,LEARNRATE:lr})\n",
    "    if i%5==0:\n",
    "        mask=np.random.choice(TE_FEA.shape[0],128,replace=False)\n",
    "        x_,y_=TE_FEA[mask],TE_LAB[mask]\n",
    "        loss_,acc_,m_=sess.run([loss,accuracy,merge],feed_dict={FX:x_,Y:y_,KEEP_PROB:1.,LEARNRATE:lr})\n",
    "        writer_.add_summary(m_,i)\n",
    "        if acc_>0.8:\n",
    "            lr=max(0.99*lr,1e-5)\n",
    "            print ('epoch {},learning rate:{}'.format(i,lr))\n",
    "        print('--epoch:{},test loss:{},test accuracy:{}'.format(i,loss_,acc_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.close()\n",
    "writer_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
